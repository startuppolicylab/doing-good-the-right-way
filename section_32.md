# 3.2 Embedded, Implicit Bias in AlgorithmsEven when analytic methods work on a technical level, sometimes these approaches can serve to embed or amplify biases that existed outside the scope of the data science project, or that were engendered by earlier decisions regard- ing the sufficiency of data sets. For example, a great deal of modern data science relies upon information culled from Twitter and Facebook. While this can be useful and pow- erful, it does still focus upon information individuals willing yield up to social media. This entirely neglects informa- tion that does not exist on social media or is held by those without access to these networks for social or economic rea- sons [5]. Equivalently, the city of Boston developed a smart- phone app that uses the phonesâ€™ accelerometers and GPS systems to report on potholes automatically [26]. On its face, this appears to be a perfect use of data science; how- ever, it neglects to account for the fact that in poorer areas of Boston, there may be fewer people driving, or who have access to smartphones in the first place. The city would necessarily receive fewer reports of potholes, not because the roads are better, but because the density of sensors is lower. Additionally, the geolocation aspects of the program introduced significant privacy and warrantless surveillance questions, as even the United States Supreme Court has sug- gested that GPS tracking constitutes a form of surveillance under the Fourth Amendment [21, Sotomayor, J. concur- ring]. Recognizing this kind of impact requires examining the data science through not just the lens of computer sci- ence, but also through economics, sociology, and law.Implicit bias does not merely crop up in a few, disparate applications of data science. According to a report produced by the White House, the statistical nature of big data analy- sis can serve to smooth over precisely the sorts of person-by- person discrimination that decades of civil rights laws have attempted to resolve [20]. This is made doubly challenging because the nature of these programs, and the specific archi- tecture of the algorithms employed in them, are often opaque to the outside world. At the moment, anti-discrimination laws are not sufficiently developed to address the impact the harms that these hidden biases can inflict [3], so the respon- sibility falls to data scientists (preferably working alongside NGOs or other advisory groups) to take reasonable steps to mitigate this impact.