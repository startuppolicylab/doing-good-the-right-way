# 2. THE TYRANNY OF THE CENTRAL LIMIT THEOREMIn the early days of “big data,” Wired’s Chris Anderson hailed it as “the end of theory [1].” With enough informa- tion, so the idea went, and with enough processing power, it would be possible to map out all of the connections within a dataset and algorithmically derive deep truths from within it, without all the messiness of hypotheses and experiments.The reality has turned out to be quite a bit more prosaic [5]. Data science, like any other science, is only as good as the information available and the techniques that we use to an- alyze it. This is not merely a philosophical conclusion. It is a fact of the mathematics that informs the entire field of data science.One of the most important ideas in all of statistics is some- thing called the Central Limit Theorem. Put simply, this theorem states that given a large enough body of indepen- dent events that all behave in roughly the same way, we can ignore small variations across them and make certain gen- eral statements about the body as a whole. This predictive power does not come for free. A logical result of the cen- tral limit theorem is that, without a sufficiently large set of well-behaved and well-understood data, we cannot trust the outcomes of statistical methods. This kind of unpredictability can have dire consequences when we attempt to apply these methods to real world problems. Additionally, it is not always clear how our results might diverge from reality. With every layer of analysis that we apply to a data set, different insufficiencies could be amplified in different ways, without any mechanism for self-correction.By way of example, consider Facebook’s controversial “fil- ter bubble” study [2]. Here, the researchers attempted to determine the extent to which Facebook usage—and by ex- tension, social media usage in general—isolates individuals from opposing viewpoints. While the Facebook researchers did find some evidence of this effect, the results of the study are less instructive than the dramatic backlash from the sci- entific community that followed [29] [10].Researchers and experts in the field of social media and big data excoriated the study, highlighting the limited un- derlying data set. Facebook’s researchers focused on indi- viduals who self-reported their political preferences, which comprises only 4% of the Facebook user base [23]. Tufekci argues, “The research was conducted on a small, skewed sub- set of Facebook users who chose to self-identify their politi- cal affiliation on Facebook and regularly log on to Facebook, about ∼4% of the population available for the study. This is super important because this sampling confounds the de- pendent variable [29].” Put another way, the Facebook re- searchers’ data set did not satisfy the requirements of the central limit theorem, so there is no reason to believe that the outcome of the result was meaningful to the larger pop- ulation of Facebook users in any sort of rigorous, statistical way [9].On top of its sampling issues, Facebook also framed its research to minimize (some might say obfuscate) the impact of Facebook’s own display algorithms on the outcome of the experiment [13]. User interactions with Facebook are en- tirely mediated by the design of Facebook’s website, which in turn is constructed from countless design choices made by hundreds or thousands of engineers, and that vary over time. This means that it is impossible to fully separate the role user decisions played from the role that Facebook’s own design choices played. Along with the relatively tiny sam- ple size, this knot of cause-and-effect serves to further cast doubt upon the explanatory power of Facebook’s study.The challenges of selecting a representative sample and of separating the behavior of the sample from external fac- tors are universal across all of modern data science. For a more concrete example of how potentially dangerous these complexities can be, we can look to a recent study concerning Google’s Flu Trends program (GFT), something that many groups have treated as a reliable source of epidemio- logical information [14]. Google created Flu Trends to pre- dict the spread of the flu based upon algorithmic analysis of searches its users made over time. The idea is straightfor- ward: many individuals who are suffering the symptoms of an illness search for them using Google’s search engine. By tracking certain key words that map to the symptoms of the flu, GFT attempted to map the spread of the disease in real time, with the hope that it would be both faster and more accurate than those produced by government organizations like the Center for Disease Control and Prevention, which relies upon laboratory reports for its estimates.However, in February 2013, Google Flu Trends predicted more than twice the number of doctor visits for “influenza- like illnesses,” and was discovered to have been overshooting the CDC’s estimates for years. Lazer et al formalized two complementary sources that contributed to these errors: in their words, “big data hubris” and “algorithm dynamics [14].” Big data hubris refers to the idea that sufficiently large-scale data science can effectively substitute for traditional data collection and analysis. In a sense, this is the applied ver- sion of Anderson’s “end of theory” idea. The concept of algo- rithm dynamics is a more subtle one, but no less important. The core technology that enabled GFT was Google’s search algorithm itself. Over time, Google modifies its algorithm to return more accurate search results to its users. However, it seems probable that at least some of these changes, made for reasons completely separate from the operation of GFT, nonetheless impacted GFT’s accuracy.While big data hubris and algorithm dynamics appear to have been the cause of GFT’s decreased efficacy, it is vital to recognize that their specific role—the doubling of predicted doctor visits—would have been nearly impossible to predict from the data and algorithms alone. It was the combined effect of numerous factors that produced this skewed result, and, as in all other things, complexity clouds predictions.Taken together, these studies highlight certain challenges that exist in any application of modern data science. Find- ing a representative data set can be difficult even for com- panies like Facebook or Google, who track nearly everything their users do on the internet. When it comes to the real world, information is not nearly so readily available, and the choices that we make in deciding how to apply the tools of data science are all the more important. The more complex a system is, the more difficult it is to understand all of the factors that may operate as hidden variables that can skew the results of an analysis. The decisions that we make in the process of analyzing data are equally important, espe- cially if there is some interaction between our analysis the underlying data set, and this is especially true when we move beyond the limited set of factors that inform online behavior into the broader, physical world.History bears this out. When it comes to applying the methods of data science in the real world, failures like those suffered by Google and Facebook happen with some regular- ity. The Rand Corporation’s attempt to redraw the routes used by New York City firefighters contributed to the de- struction caused by a decade of fires that destroyed much of the Bronx and killed thousands of people [8]. “Data-driven” approaches to policing, from “broken windows policing” [11] (a method that proposed focusing police work on low-level disturbances would prevent more serious crimes from occurring) to CompStat (a system and philosophy organizing po- lice work that frequently relies upon the application of tech- nology) have resulted in decades of debate and few unequiv- ocal gains [15]. And at the highest levels of government, the attempts to construct a terrorist screening database has been plagued by errors, overcollection [22], and missed op- portunities [24], and the National Security Agency’s meta- data collection and processing efforts were found to be inef- fective [4]. Schneier has argued that any attempts to “con- nect the dots” for counterterrorism purposes are doomed to fail, as it is a problem that is fundamentally unsolvable by conventional data analytic techniques [25].While these high-profile cases may seem to suggest that applying data science in the service of social good is an impossibility, the real lesson is somewhat more subtle. In terms of the mathematical ideas described above, all of these examples relied on oversimplified models of complex social phenomena (Lazer’s “big data hubris”), with apparently few institutional backstops to ensure that these applications of data science were employed correctly and rigorously. In or- der to avoid these types of outcomes in the future, this is one area where we might wish to focus our efforts.The central limit theorem does not teach us that overly complex systems are somehow invulnerable to statistical anal- ysis; rather, it shows that the more complex a system is, the more careful we need to be in choosing our data set, the ques- tions we ask of it, and the methods that we use to answer those questions. This is an area where an interdisciplinary approach is particularly helpful. Data scientists may under- stand the difference between good data and bad data, but they often lack the subject-area expertise of a given issue that others, such as those who work with NGOs or govern- ments, might have gained through long experience. This would serve dual, and mutually beneficial, purposes. On the one hand, it would help data scientists to understand the problems in question more clearly, and help prevent the effects of big data hubris, and mitigate the potential im- pacts of algorithm dynamics. On the other hand, it would help those who are not steeped in arcane data science to better understand what types of questions these analytical methods are capable of solving, and perhaps help propose solutions to issues had long been considered intractable.