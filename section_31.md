# 3.1 The Impracticality of True AnonymizationFor some time now, it has been clear to computer scien- tists that the larger a data set is, the more difficult it is to anonymize it effectively [17]. While some of this is due to technical advances that aid interpolation and inference, some of it simply arises from the fact that human intuition is not particularly good at comprehending how quickly mul- tiple data points can work together to uniquely identify an individual. For example, as early as 2000, Sweeney demon- strated that 87% of the U.S. population can be uniquely re-identified based on five-digit ZIP code, gender, and date of birth [27]. Researchers have also shown that as few as four cellular tower check-ins from our phones are enough to uniquely identify 95% of individuals because location infor- mation is so unique [6]. This inability to anonymize is not a function of laziness on the part of researchers, but a fact of the underlying mathematics and probabilities that govern large data sets.This problem can be multiplied when organizations rely upon technologies that they believe secures anonymity, but falls in the face of reidentification attacks. Recently, the New York City Taxi and Limousine Commission released its 2013 dataset to a researcher in response to a Freedom of Information Law request [30]. It quickly became clear that the techniques they used to anonymize the data were woe- fully insufficient [19]. Moreover, when combined with im- ages found online, it was possible to learn even more from the data set, including the movement habits and locations of a number of celebrities [28]. To be fair, it is entirely appro- priate for New York City to collect this sort of information, as its analysis can yield rich insights about traffic flow, the economics of livery services, commuting patterns, along with countless others. These are the sorts of results that could be of great value to many disparate groups, so releasing the in- formation is far from an obvious mistake. The troubles arise from the technical deficiencies in their anonymization pro- tocols. That said, there are provable, technical solutions— specifically, differential privacy [7]—that can both preserve privacy and allow for the analysis of the data set.The implications of the failure of anonymization are pro- found. At the highest level, it suggests that the conventional approach, creating special categories for sensitive data (so- cial security numbers, medical histories, and the like), is not necessarily sufficient in the face of easy reidentification [18]. More deeply, however, it shows that there will always be a tension between the good that can be generated by analyz- ing a particular dataset and the dangers that compiling that dataset can introduce. As a general rule, many nonprofits and other non-expert organizations (and even some govern- ments) tend to err on the side of collecting more data, not wanting to foreclose possible future uses, rather than taking a more minimalist, risk-averse approach. This is especially true in cases where the potential harms are far in the future, or are otherwise difficult to calculate.There is a great deal of uncertainty here, as well, for it is almost impossible to look at a particular set of data and im- mediately deduce all of the possible nefarious uses to which it might be put. At the same time, it is necessary for or- ganizations to ask these sorts of questions as part of their data science program development process. Narayanan et al have argued a kind of precautionary approach is needed to“big data privacy” that places a heavier burden upon data releasers to ensure that the anonymity of their data setsis preserved [16]. Unfortunately, this approach comes with higher technical costs and requires specific expertise. Many local governments and smaller NGOs lack the experience or the knowledge to know that this expertise is necessary, and, in many cases, lack the resources to access that knowledge even if they understand its value.